{"cells":[{"cell_type":"markdown","metadata":{},"source":["# INFO8010: A Yolo v3 object detector implementation"]},{"cell_type":"markdown","metadata":{},"source":["The weights can be found here [git@github.com:SkYF4Il/DP_YOLO_V3.git](https://github.com/SkYF4Il/DP_YOLO_V3.git)"]},{"cell_type":"markdown","metadata":{},"source":["# Step 1 : Import the necessary libraries and define the global variables needed in the notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from torchvision.transforms import ToPILImage\n","from torch.utils.data import DataLoader, Dataset\n","import random\n","import pickle as pkl\n","import os\n","import pandas as pd\n","import numpy as np\n","from PIL import Image\n","import torch\n","import torchvision\n","from torchvision import transforms\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","import torch.nn as nn\n","from rich.progress import Progress, BarColumn, TextColumn, TimeRemainingColumn, TimeElapsedColumn\n","from rich.console import Console\n","import time\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","import warnings\n","from collections import defaultdict\n","import gc"]},{"cell_type":"markdown","metadata":{},"source":["## Useful global variables"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the device we are using (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Ignore warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Enable cuDNN benchmark mode for performance optimization.\n","# This allows the cuDNN library to optimize based on the hardware and model, potentially improving training speed.\n","torch.backends.cudnn.benchmark = True\n","\n","# Define the image size for the input to the model.\n","# The YOLO model typically uses a square input size (e.g., 416x416 pixels).\n","image_size = 416\n","\n","# Define the number of steps for the training.\n","# This is how many times the model will iterate over the entire training dataset.\n","num_steps = 30\n","\n","# Define the grid size for the YOLO model.\n","# YOLO uses a grid to divide the image into regions where object detection predictions are made.\n","grid_size = [image_size // 32, image_size // 16, image_size // 8]\n","\n","# Define the anchors for the YOLO model taken from the YOLOv3 paper.\n","# Anchors are predefined bounding box sizes that help the model predict objects of various scales.\n","anchors = [\n","    [(116, 90), (156, 198), (373, 326)],  # large scale\n","    [(30, 61), (62, 45), (59, 119)],      # medium scale\n","    [(10, 13), (16, 30), (33, 23)]        # small scale\n","]\n","\n","# Normalize each anchor by dividing by the image size (416).\n","# This scales the anchors relative to the size of the input image.\n","normalized_anchors = [\n","    [(w / image_size, h / image_size) for (w, h) in scale] for scale in anchors\n","]\n","\n","# Convert the normalized anchors to a tensor and move them to the chosen device (GPU/CPU).\n","normalized_anchors2 = torch.tensor(normalized_anchors).to(device)\n","\n","# Define the scaled anchors.\n","# The anchors are further scaled according to the grid size.\n","scaled_anchors = normalized_anchors2 / (\n","    1 / torch.tensor(grid_size).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",").to(device)\n","\n","# Define the batch size for training.\n","# This is the number of images processed together in one forward/backward pass.\n","batch_size = 32\n","\n","# Define the learning rate for training.\n","# This controls how much to adjust the model's parameters in response to the estimated error.\n","learning_rate = 1e-4\n","\n","# Define the weight decay for the optimizer.\n","# This is a regularization technique used to prevent overfitting by penalizing large weights.\n","weight_decay = 1e-4\n","\n","# Define the number of workers for loading data.\n","# This determines how many subprocesses to use for data loading.\n","num_workers = 4\n","\n","# Set whether to pin memory in data loaders.\n","# Pinning memory can speed up data transfer to the GPU.\n","pin_memory = True\n","\n","# Set whether to save the model after training.\n","save_model = True\n","\n","# Set whether to load a pre-trained model.\n","load_model = False\n","\n","# set whether to resume training or reset from start\n","resume_training = True\n","\n","# Define the file name for saving the model snapshot.\n","snapshot_file = \"snapshot2_VOC.pth.tar\"\n","\n","# Define the Intersection over Union (IoU) threshold for evaluating detections.\n","# IoU measures the overlap between the predicted bounding box and the ground truth.\n","iou_threshold = 0.5\n","\n","# Define the confidence threshold for detections.\n","# This sets the minimum confidence level for considering a detection as valid.\n","conf_threshold = 0.7\n","\n","# The IoU (Intersection over Union) threshold used to determine whether a predicted bounding box \n","# should be ignored during training. If the IoU between a predicted box and any ground truth box \n","# exceeds this value, the predicted box is ignored in the loss calculation, \n","# as it is considered a sufficiently good prediction (value taken from the cfg file of YOLOv3 given on the official site)\n","ignore_thresh = 0.7\n","\n","# Define the root directory for the dataset.\n","root_dir = '/kaggle/input/pascal-voc/PASCAL_VOC' # To use PASCAL_VOC dataset\n","#root_dir = '/kaggle/input/coco-dataset/COCO'    # To use COCO dataset\n","\n","# Set paths based on the chosen dataset.\n","# This block checks if the dataset is PASCAL VOC or COCO and sets paths accordingly.\n","if root_dir == '/kaggle/input/coco-dataset/COCO':\n","    names_path = '/kaggle/input/data-dataset/data/coco.names'\n","elif root_dir == '/kaggle/input/pascal-voc/PASCAL_VOC':\n","    names_path = '/kaggle/input/data-dataset/data/voc.names'\n","\n","train_csv_path = \"train.csv\"\n","val_csv_path = \"val.csv\"\n","test_csv_path = \"test.csv\"\n","\n","# Load the color palette for visualizing bounding boxes.\n","# This palette is used to color the boxes drawn around detected objects.\n","colors = pkl.load(open(\"/kaggle/input/data-dataset/data/pallete\", \"rb\"))"]},{"cell_type":"markdown","metadata":{},"source":["# Step 2 : Define some utility function "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to calculate the Intersection over Union (IoU) score\n","def compute_iou_boxes(box1, box2):\n","        # Extract center coordinates and dimensions\n","        center_x1, center_y1, width1, height1 = box1[..., 0:1], box1[..., 1:2], box1[..., 2:3], box1[..., 3:4]\n","        center_x2, center_y2, width2, height2 = box2[..., 0:1], box2[..., 1:2], box2[..., 2:3], box2[..., 3:4]\n","\n","        # Calculate corners of the bounding boxes\n","        b1_x1 = center_x1 - width1 / 2\n","        b1_y1 = center_y1 - height1 / 2\n","        b1_x2 = center_x1 + width1 / 2\n","        b1_y2 = center_y1 + height1 / 2\n","\n","        b2_x1 = center_x2 - width2 / 2\n","        b2_y1 = center_y2 - height2 / 2\n","        b2_x2 = center_x2 + width2 / 2\n","        b2_y2 = center_y2 + height2 / 2\n","\n","        # Get the coordinates of the intersection rectangle\n","        x1 = torch.max(b1_x1, b2_x1)\n","        y1 = torch.max(b1_y1, b2_y1)\n","        x2 = torch.min(b1_x2, b2_x2)\n","        y2 = torch.min(b1_y2, b2_y2)\n","\n","        # Calculate the area of intersection rectangle (clamping the negative values to 0)\n","        intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n","\n","        # Compute the area of both the prediction and ground-truth rectangles\n","        box1_area = abs((b1_x2 - b1_x1) * (b1_y2 - b1_y1))\n","        box2_area = abs((b2_x2 - b2_x1) * (b2_y2 - b2_y1))\n","        union = box1_area + box2_area - intersection\n","\n","        # Compute the Intersection over Union (IoU) score\n","        iou_score = intersection / (union + 1e-6)\n","\n","        return iou_score"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to calculate the Intersection over Union (IoU) score\n","# When boxes contain dimensions directly\n","def compute_iou_sizes(box1, box2):\n","        # Compute the intersection width and height\n","        inter_width = torch.min(box1[..., 0], box2[..., 0])\n","        inter_height = torch.min(box1[..., 1], box2[..., 1])\n","        inter_area = inter_width * inter_height\n","\n","        # Compute the area of both boxes\n","        box1_area = box1[..., 0] * box1[..., 1]\n","        box2_area = box2[..., 0] * box2[..., 1]\n","        union_area = box1_area + box2_area - inter_area\n","\n","        # Compute the IoU score\n","        iou_score = inter_area / union_area\n","\n","        return iou_score"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# NB : This function is not very used but it is our functional hand implementation of NMS \n","# that is why we kept it in our notebook. Nevertheless since it uses a loop rather than tensor operations\n","# it is much slower than the next impelementation that uses the torchvision package\n","\n","# Function to apply non-maximum suppression (NMS) to bounding boxes\n","def apply_nms_manual(bboxes, iou_thresh, conf_thresh):\n","    # Retain boxes that meet the confidence threshold\n","    filtered_boxes = [box for box in bboxes if box[1] > conf_thresh]\n","\n","    # Sort boxes based on descending confidence\n","    filtered_boxes.sort(key=lambda x: x[1], reverse=True)\n","\n","    # Store final bounding boxes after non-maximum suppression\n","    final_boxes = []\n","\n","    while filtered_boxes:\n","        # Take the box with the highest confidence as the primary box\n","        highest_conf_box = filtered_boxes.pop(0)\n","\n","        # Add this primary box to the result set\n","        final_boxes.append(highest_conf_box)\n","\n","        # Define a new list of boxes to keep (exclude those with high IoU with the current box)\n","        filtered_boxes = [\n","            box for box in filtered_boxes\n","            if box[0] != highest_conf_box[0] or compute_iou_boxes(\n","                torch.tensor(highest_conf_box[2:]), torch.tensor(box[2:])\n","            ) < iou_thresh\n","        ]\n","\n","    return final_boxes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# This implementation even though it is not hand made it seems to be much faster than the previous one\n","\n","def apply_nms(bboxes, iou_thresh, conf_thresh):\n","    # Convert bounding boxes to Tensor\n","    boxes = torch.tensor([box[2:] for box in bboxes], dtype=torch.float32).to(device)\n","    scores = torch.tensor([box[1] for box in bboxes], dtype=torch.float32).to(device)\n","    indices = torchvision.ops.nms(boxes, scores, iou_thresh)\n","\n","    # Filter out boxes that don't meet the confidence threshold\n","    final_boxes = [bboxes[i] for i in indices if bboxes[i][1] > conf_thresh]\n","    return final_boxes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def transform_boxes(tensor):\n","    \"\"\"This function transforms a tensor representing bounding boxes and their associated class labels\n","    by rearranging and reshaping it into a specific format required for further processing.\"\"\"\n","    \n","    # Check if the tensor is empty\n","    if tensor.numel() == 0:\n","        return torch.empty((0, 5))  # Return an empty tensor with the expected dimensions (1 batch, 5 columns)\n","\n","    # Step 1: Extract the necessary parts of the tensor\n","    class_label = tensor[:, 0]  # Extract the class label from the first column of each row\n","    coordinates = tensor[:, 2:] # Extract the bounding box coordinates (x, y, w, h) from columns 2 onward\n","\n","    # Step 2: Concatenate the parts together in the desired order\n","    transformed_tensor = torch.cat([class_label.unsqueeze(1), coordinates], dim=1)\n","    # The class label is unsqueezed to add a dimension, making it a column vector,\n","    # and then concatenated with the coordinates along the columns to form a new tensor\n","\n","    # Step 3: Add an extra dimension to match the target shape\n","    transformed_tensor = transformed_tensor.unsqueeze(0)\n","    # A new dimension is added at the beginning, typically to match a target shape required for batch processing\n","\n","    return transformed_tensor  # The transformed tensor is returned for further use"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def load_classes(namesfile):\n","    \"\"\"Utility function to load class names from a file.\"\"\"\n","    with open(namesfile, \"r\") as file:\n","        names = file.read().strip().split(\"\\n\")\n","    return names"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def display_image_with_boxes(data, class_labels):\n","    \"\"\"Displays images with bounding boxes using Matplotlib.\"\"\"\n","\n","    # Check if data is empty or if there are no images or bounding boxes\n","    if not data or 'image' not in data or 'bboxes' not in data:\n","        print(\"data does not contain right format\")\n","        return  # Do nothing if the data is empty or lacks required keys\n","    \n","    if len(data['image']) == 0 or len(data['bboxes']) == 0:\n","        print(\"There is no image or box to display\")\n","        return  # Do nothing if there are no images or bounding boxes\n","\n","    # Step 1: Extract the image tensor and bounding boxes from the data\n","    image_tensor = data['image'][0]  # Extract the first image tensor from the batch\n","    bboxes = data['bboxes'][0]       # Extract the corresponding bounding boxes for the first image\n","\n","    # If the extracted image tensor or bounding boxes are empty, return early\n","    if image_tensor.numel() == 0 or len(bboxes) == 0:\n","        return  # Do nothing if the image or bounding boxes are empty\n","\n","    # Step 2: Convert the image tensor to a PIL image, then to a NumPy array for display\n","    image = ToPILImage()(image_tensor)  # Convert the image tensor to a PIL image\n","    image = np.array(image)             # Convert the PIL image to a NumPy array for display\n","\n","    # Step 3: Set up Matplotlib figure and axis to display the image\n","    fig, ax = plt.subplots(1)  # Create a new figure and axis\n","    ax.imshow(image)           # Display the image on the axis\n","    img_height, img_width, _ = image.shape  # Get the image dimensions (height, width, channels)\n","\n","    # Step 4: Generate random colors for each class label for bounding box display\n","    colors = [(random.random(), random.random(), random.random()) for _ in range(len(class_labels))]\n","    # Generate a random RGB color for each class label\n","\n","    # Step 5: Loop through each bounding box and display it on the image\n","    for bbox in bboxes:\n","        class_index = int(bbox[0])           # Get the class index from the bounding box\n","        class_name = class_labels[class_index]  # Get the class name using the index\n","        color = colors[class_index]          # Get the color corresponding to the class label\n","\n","        # Step 6: Convert the bounding box center coordinates and dimensions to rectangle format\n","        x_center, y_center, w, h = bbox[1:]  # Extract the center coordinates (x, y) and dimensions (w, h)\n","        x = x_center * img_width - w * img_width / 2  # Calculate the x-coordinate of the top-left corner\n","        y = y_center * img_height - h * img_height / 2  # Calculate the y-coordinate of the top-left corner\n","\n","        # Step 7: Create a rectangle patch for the bounding box and add it to the axis\n","        rect = patches.Rectangle(\n","            (x, y), w * img_width, h * img_height, linewidth=2, edgecolor=color, facecolor='none'\n","        )\n","        ax.add_patch(rect)  # Add the rectangle patch to the axis\n","\n","        # Step 8: Add class label text to the bounding box\n","        font_size = np.clip(h * img_height / 25, 8, None)  # Dynamically set the font size based on box height\n","        vertical_alignment = 'top'\n","        horizontal_alignment = 'left'\n","        padding = 3  # Add some padding to the text position\n","        x_text = x + padding\n","        y_text = y + padding if y + padding > 0 else 0\n","\n","        # Add text label with background color matching the bounding box\n","        ax.text(\n","            x_text, y_text, class_name, color='white', fontsize=font_size,\n","            va=vertical_alignment, ha=horizontal_alignment,\n","            bbox=dict(facecolor=color, edgecolor=color, boxstyle='round,pad=0.1')\n","        )\n","\n","    # Step 9: Remove axis ticks and display the final image with bounding boxes\n","    plt.axis('off')  # Turn off axis ticks and labels\n","    plt.show()       # Display the image with bounding boxes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# This function converts the output of a YOLO model from grid coordinates to bounding boxes.\n","# It processes the output tensor, which contains confidence scores, class predictions,\n","# and bounding box attributes, and adjusts these values relative to the grid dimensions.\n","# The function returns the bounding boxes along with their associated class and confidence scores\n","# in a format that is easier to interpret for further processing or visualization.\n","\n","def grid_to_bboxes(output, anchors, grid_dim):\n","    # Determine the number of batches and anchors\n","    batch_count = output.size(0)\n","    num_anchors = len(anchors)\n","\n","    # Split the output into their respective components\n","    confidence_scores = output[..., 0:1]\n","    class_predictions = output[..., 5:6]\n","    bbox_attributes = output[..., 1:5]\n","\n","    # Generate grid offsets for x and y directions\n","    grid_y, grid_x = torch.meshgrid(torch.arange(grid_dim), torch.arange(grid_dim))\n","    grid_x, grid_y = grid_x.to(output.device), grid_y.to(output.device)\n","\n","    # Expand dimensions for broadcasting with bbox_attributes\n","    grid_x = grid_x.view(1, 1, grid_dim, grid_dim, 1).repeat(batch_count, num_anchors, 1, 1, 1)\n","    grid_y = grid_y.view(1, 1, grid_dim, grid_dim, 1).repeat(batch_count, num_anchors, 1, 1, 1)\n","\n","    # Compute the adjusted x and y coordinates\n","    x_adj = (bbox_attributes[..., 0:1] + grid_x) / grid_dim\n","    y_adj = (bbox_attributes[..., 1:2] + grid_y) / grid_dim\n","\n","    # Calculate width and height relative to the grid\n","    box_wh = bbox_attributes[..., 2:4] / grid_dim\n","\n","    # Combine the adjusted coordinates, dimensions, class predictions, and confidence scores\n","    final_bboxes = torch.cat([class_predictions, confidence_scores, x_adj, y_adj, box_wh], dim=-1)\n","\n","    # Reshape the tensor to the desired shape\n","    final_bboxes = final_bboxes.view(batch_count, num_anchors * grid_dim * grid_dim, 6)\n","\n","    # Convert to list format for output\n","    return final_bboxes.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# This function converts the predictions from a YOLO model into bounding boxes. \n","# It processes the output tensor, which contains bounding box attributes, confidence scores,\n","# and class predictions, and adjusts these values relative to the grid dimensions and anchor boxes.\n","# The function applies transformations to ensure the bounding boxes are in the correct format\n","# and scales them appropriately. The output is a list of bounding boxes with associated class \n","# predictions and confidence scores, ready for further processing or evaluation.\n","\n","def pred_to_bboxes(output, anchors, grid_dim):\n","    # Determine the number of batches and anchors\n","    batch_count = output.size(0)\n","    num_anchors = len(anchors)\n","    bbox_attributes = output[..., 1:5]\n","    \n","    # Apply the sigmoid function to the x and y coordinates to constrain them between 0 and 1,\n","    # which makes them relative to the grid cell. \n","    # Apply the exponent function to the width and height to ensure they are positive and scale them\n","    # by the corresponding anchor box dimensions. Then, compute the confidence scores and determine \n","    # the best predicted class for each box.\n","\n","    anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n","    bbox_attributes[..., 0:2] = torch.sigmoid(bbox_attributes[..., 0:2])\n","    bbox_attributes[..., 2:] = torch.exp(bbox_attributes[..., 2:]) * anchors\n","    confidence_scores = torch.sigmoid(output[..., 0:1])\n","    class_predictions = torch.argmax(output[..., 5:], dim=-1, keepdim=True)\n","\n","    # Generate grid offsets for x and y directions\n","    grid_y, grid_x = torch.meshgrid(torch.arange(grid_dim), torch.arange(grid_dim))\n","    grid_x, grid_y = grid_x.to(output.device), grid_y.to(output.device)\n","\n","    # Expand dimensions for broadcasting with bbox_attributes\n","    grid_x = grid_x.view(1, 1, grid_dim, grid_dim, 1).repeat(batch_count, num_anchors, 1, 1, 1)\n","    grid_y = grid_y.view(1, 1, grid_dim, grid_dim, 1).repeat(batch_count, num_anchors, 1, 1, 1)\n","\n","    # Compute the adjusted x and y coordinates\n","    x_adj = (bbox_attributes[..., 0:1] + grid_x) / grid_dim\n","    y_adj = (bbox_attributes[..., 1:2] + grid_y) / grid_dim\n","\n","    # Calculate width and height relative to the grid\n","    box_wh = bbox_attributes[..., 2:4] / grid_dim\n","\n","    # Combine the adjusted coordinates, dimensions, class predictions, and confidence scores\n","    final_bboxes = torch.cat([class_predictions, confidence_scores, x_adj, y_adj, box_wh], dim=-1)\n","\n","    # Reshape the tensor to the desired shape\n","    final_bboxes = final_bboxes.view(batch_count, num_anchors * grid_dim * grid_dim, 6)\n","\n","    # Convert to list format for output\n","    return final_bboxes.tolist()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def calculate_accuracy(predictions, targets, mask):\n","    \"\"\"\n","    Computes the classification accuracy by comparing the predicted classes \n","    to the target classes for the objects identified by the mask.\n","    \"\"\"\n","    correct_predictions = torch.sum(torch.argmax(predictions[mask], dim=-1) == targets[mask])\n","    total_predictions = torch.sum(mask)\n","    return correct_predictions.detach(), total_predictions.detach()\n","\n","def calculate_objectness_accuracy(predictions, targets, mask, threshold):\n","    \"\"\"\n","    Computes the accuracy of objectness predictions by thresholding the predictions\n","    and comparing them to the target values for the objects identified by the mask.\n","    \"\"\"\n","    objectness_predictions = torch.sigmoid(predictions) > threshold\n","    correct_predictions = torch.sum(objectness_predictions[mask] == targets[mask])\n","    total_predictions = torch.sum(mask)\n","    return correct_predictions.detach(), total_predictions.detach()\n","\n","def calculate_coord_accuracy(predictions, targets, mask, iou_threshold):\n","    \"\"\"\n","    Computes the coordinates accuracy by comparing IoU of predicted and target boxes.\n","    Returns the number of correct predictions where IoU exceeds the threshold.\n","    \"\"\"\n","    # Compute IoUs\n","    iou_scores = compute_iou_boxes(predictions[mask][..., :4], targets[mask][..., :4])\n","\n","    # Determine the number of predictions with IoU greater than the threshold\n","    correct_predictions = torch.sum(iou_scores > iou_threshold)\n","    total_predictions = torch.sum(mask)\n","\n","    return correct_predictions.detach(), total_predictions.detach()\n","\n","def evaluate_model_accuracy(model, data_loader, threshold, iou_threshold=0.5):\n","    # Set the model to evaluation mode (disables dropout, batch norm, etc.)\n","    model.eval()\n","    \n","    # Initialize a dictionary to store accuracy metrics for different predictions\n","    accuracy_metrics = {\n","        \"correct_class_predictions\": 0,      # Correct class predictions\n","        \"total_class_predictions\": 0,        # Total class predictions\n","        \"correct_object_predictions\": 0,     # Correct objectness predictions (cells with objects)\n","        \"total_object_predictions\": 0,       # Total objectness predictions (cells with objects)\n","        \"correct_noobject_predictions\": 0,   # Correct no-object predictions (cells without objects)\n","        \"total_noobject_predictions\": 0,     # Total no-object predictions (cells without objects)\n","        \"correct_coord_predictions\": 0,      # Correct coordinate predictions (IoU above threshold)\n","        \"total_coord_predictions\": 0,        # Total coordinate predictions (cells with objects)\n","    }\n","\n","    # Loop through each batch in the data loader\n","    for batch_index, batch in enumerate(data_loader):\n","        images = batch['image']              # Extract images from the batch\n","        labels = batch['outputs']            # Extract ground truth labels from the batch\n","        images = images.to(device)           # Move images to the appropriate device (GPU/CPU)\n","\n","        # Disable gradient calculation for evaluation\n","        with torch.no_grad():\n","            predictions = model(images)      # Perform forward pass to get model predictions\n","\n","            # Iterate over the three scales (e.g., 13x13, 26x26, 52x52) of the model's output\n","            for scale_index in range(3):\n","                labels[scale_index] = labels[scale_index].to(device)  # Move labels to device\n","                object_mask = labels[scale_index][..., 0] == 1        # Mask for cells with objects\n","                noobject_mask = labels[scale_index][..., 0] == 0      # Mask for cells without objects\n","\n","                # Calculate and accumulate class accuracy for cells with objects\n","                correct_preds, total_preds = calculate_accuracy(\n","                    predictions[scale_index][..., 5:],               # Predicted classes\n","                    labels[scale_index][..., 5],                      # Ground truth classes\n","                    object_mask                                       # Mask for objects\n","                )\n","                accuracy_metrics[\"correct_class_predictions\"] += correct_preds\n","                accuracy_metrics[\"total_class_predictions\"] += total_preds\n","\n","                # Calculate and accumulate objectness accuracy for cells with objects\n","                correct_preds, total_preds = calculate_objectness_accuracy(\n","                    predictions[scale_index][..., 0],                 # Predicted objectness\n","                    labels[scale_index][..., 0],                      # Ground truth objectness\n","                    object_mask,                                      # Mask for objects\n","                    threshold                                         # Objectness threshold\n","                )\n","                accuracy_metrics[\"correct_object_predictions\"] += correct_preds\n","                accuracy_metrics[\"total_object_predictions\"] += total_preds\n","\n","                # Calculate and accumulate no-object accuracy for cells without objects\n","                correct_preds, total_preds = calculate_objectness_accuracy(\n","                    predictions[scale_index][..., 0],                 # Predicted objectness\n","                    labels[scale_index][..., 0],                      # Ground truth objectness\n","                    noobject_mask,                                    # Mask for no-objects\n","                    threshold                                         # Objectness threshold\n","                )\n","                accuracy_metrics[\"correct_noobject_predictions\"] += correct_preds\n","                accuracy_metrics[\"total_noobject_predictions\"] += total_preds\n","\n","                # Calculate and accumulate coordinates accuracy for cells with objects\n","                correct_preds, total_preds = calculate_coord_accuracy(\n","                    predictions[scale_index][..., 1:5],               # Predicted coordinates (x, y, w, h)\n","                    labels[scale_index][..., 1:5],                    # Ground truth coordinates (x, y, w, h)\n","                    object_mask,                                      # Mask for objects\n","                    iou_threshold                                     # IoU threshold for accuracy\n","                )\n","                accuracy_metrics[\"correct_coord_predictions\"] += correct_preds\n","                accuracy_metrics[\"total_coord_predictions\"] += total_preds\n","\n","    # Calculate overall accuracy metrics for the current batch\n","    class_accuracy = (accuracy_metrics[\"correct_class_predictions\"] / \n","                      (accuracy_metrics[\"total_class_predictions\"] + 1e-16)) * 100\n","    noobject_accuracy = (accuracy_metrics[\"correct_noobject_predictions\"] / \n","                         (accuracy_metrics[\"total_noobject_predictions\"] + 1e-16)) * 100\n","    object_accuracy = (accuracy_metrics[\"correct_object_predictions\"] / \n","                       (accuracy_metrics[\"total_object_predictions\"] + 1e-16)) * 100\n","    coord_accuracy = (accuracy_metrics[\"correct_coord_predictions\"] / \n","                      (accuracy_metrics[\"total_coord_predictions\"] + 1e-16)) * 100\n","\n","    # Print accuracy metrics for the current batch\n","    print(f\"Class accuracy: {class_accuracy:.2f}%\")\n","    print(f\"No-object accuracy: {noobject_accuracy:.2f}%\")\n","    print(f\"Object accuracy: {object_accuracy:.2f}%\")\n","    print(f\"Coordinates accuracy (IoU > {iou_threshold}): {coord_accuracy:.2f}%\")\n","\n","    # Set the model back to training mode after evaluation\n","    model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def save_snapshot(model, optimizer, filename, step=None, scheduler=None, extra_info=None):\n","    \"\"\"\n","    Saves the model and optimizer state dictionaries along with additional information.\n","\n","    Args:\n","        model (nn.Module): The model to save.\n","        optimizer (torch.optim.Optimizer): The optimizer to save.\n","        filename (str): The filename for the snapshot.\n","        step (int, optional): The current step number, if available. Defaults to None.\n","        scheduler (torch.optim.lr_scheduler, optional): The learning rate scheduler, if used. Defaults to None.\n","        extra_info (dict, optional): Any extra information you want to save. Defaults to None.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    print(\"=> Saving model snapshot\")\n","    \n","    snapshot = {\n","        \"state_dict\": model.state_dict(),  # Save model parameters\n","        \"optimizer_state_dict\": optimizer.state_dict(),  # Save optimizer parameters\n","    }\n","    \n","    if step is not None:\n","        snapshot[\"step\"] = step  # Save the current step number if provided\n","    \n","    if scheduler is not None:\n","        snapshot[\"scheduler_state_dict\"] = scheduler.state_dict()  # Save the scheduler state if provided\n","    \n","    if extra_info is not None:\n","        snapshot[\"extra_info\"] = extra_info  # Save any additional information if provided\n","    \n","    # Save the snapshot to the specified file\n","    torch.save(snapshot, filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def load_snapshot(snapshot_file, model, optimizer=None, scheduler=None, lr=None, device=torch.device('cpu')):\n","    \"\"\"\n","    Loads the model, optimizer, and scheduler state dictionaries from a snapshot file.\n","\n","    Args:\n","        snapshot_file (str): The path to the snapshot file.\n","        model (nn.Module): The model to load the state into.\n","        optimizer (torch.optim.Optimizer, optional): The optimizer to load the state into. Defaults to None.\n","        scheduler (torch.optim.lr_scheduler, optional): The scheduler to load the state into. Defaults to None.\n","        lr (float, optional): The learning rate to set for the optimizer. If None, the optimizer's learning rate is not modified.\n","        device (torch.device, optional): The device on which to load the model. Defaults to CPU.\n","\n","    Returns:\n","        int: The step number from the snapshot, if available, else None.\n","        dict: Extra information stored in the snapshot, if available, else None.\n","    \"\"\"\n","    print(f\"=> Loading model snapshot to {device}\")\n","    snapshot = torch.load(snapshot_file, map_location=device)\n","\n","    # Load model state\n","    model.load_state_dict(snapshot[\"state_dict\"])\n","    model.to(device)  # Ensure the model is moved to the correct device\n","    \n","    # Load optimizer state if provided\n","    if optimizer is not None and \"optimizer_state_dict\" in snapshot:\n","        optimizer.load_state_dict(snapshot[\"optimizer_state_dict\"])\n","\n","        # Reset the learning rate if a new one is provided\n","        if lr is not None:\n","            for param_group in optimizer.param_groups:\n","                param_group[\"lr\"] = lr\n","\n","    # Load scheduler state if provided\n","    if scheduler is not None and \"scheduler_state_dict\" in snapshot:\n","        scheduler.load_state_dict(snapshot[\"scheduler_state_dict\"])\n","\n","    # Return the step and any extra information if they exist\n","    step = snapshot.get(\"step\", None)\n","    extra_info = snapshot.get(\"extra_info\", None)\n","\n","    return step, extra_info"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def extract_bboxes_from_predictions(predictions, anchors, num_scales, device):\n","    \"\"\"\n","    Extracts bounding boxes from the model's predictions for each scale.\n","\n","    Args:\n","        predictions (list): List of predictions from the model, one for each scale.\n","        anchors (list): List of anchor boxes for each scale.\n","        num_scales (int): Number of scales used in the model.\n","        device (str): Device on which the operations will be performed.\n","\n","    Returns:\n","        list: List of bounding boxes for each image in the batch.\n","    \"\"\"\n","    batch_size = predictions[0].shape[0]\n","    all_bboxes = [[] for _ in range(batch_size)]\n","    \n","    for scale_idx in range(num_scales):\n","        _, _, grid_size, _, _ = predictions[scale_idx].shape\n","        anchor = torch.tensor([*anchors[scale_idx]]).to(device) * grid_size\n","\n","        # Process the entire batch at once\n","        boxes = pred_to_bboxes(predictions[scale_idx], anchor, grid_size)\n","        for img_idx, box in enumerate(boxes):\n","            all_bboxes[img_idx].extend(box)\n","    \n","    return all_bboxes\n","\n","def extract_ground_truth_bboxes(labels, anchors, scale_idx, grid_size, device):\n","    \"\"\"\n","    Extracts ground truth bounding boxes from labels for a specific scale.\n","\n","    Args:\n","        labels (list of Tensors): The labels from the DataLoader.\n","        anchors (list): List of anchor boxes for the given scale.\n","        scale_idx (int): The index of the scale to be processed.\n","        grid_size (int): The size of the grid for the given scale.\n","        device (str): Device on which to perform computations.\n","\n","    Returns:\n","        list: Ground truth bounding boxes for each image in the batch.\n","    \"\"\"\n","    # Ensure labels are on the correct device\n","    labels[scale_idx] = labels[scale_idx].to(device)\n","    \n","    # Move anchors to the same device as labels\n","    anchors = torch.tensor(anchors[scale_idx]).to(device)\n","\n","    # Convert grid-based labels to bounding boxes\n","    gt_bboxes = grid_to_bboxes(labels[scale_idx], anchors, grid_size)\n","    \n","    return gt_bboxes\n","\n","def gather_evaluation_bboxes(loader, model, iou_threshold, conf_threshold, anchors, device):\n","    \"\"\"\n","    Gathers predicted and ground truth bounding boxes for evaluation.\n","\n","    Args:\n","        loader (DataLoader): DataLoader providing batches of images and labels.\n","        model (nn.Module): The object detection model.\n","        iou_threshold (float): IoU threshold for NMS.\n","        conf_threshold (float): Confidence threshold for filtering predictions.\n","        anchors (list): List of anchor boxes used by the model.\n","        device (str): Device on which to perform computations.\n","\n","    Returns:\n","        tuple: Lists of predicted and ground truth bounding boxes.\n","    \"\"\"\n","    model.eval()  # Set the model to evaluation mode\n","    image_idx = 0\n","    predicted_bboxes = []\n","    ground_truth_bboxes = []\n","    num_scales = len(anchors)\n","    \n","    for batch in loader:\n","        images = batch['image'].to(device)\n","        labels = batch['outputs']\n","\n","        with torch.no_grad():\n","            predictions = model(images)\n","\n","            # Process the entire batch at once\n","            batch_bboxes = extract_bboxes_from_predictions(predictions, anchors, num_scales, device)\n","            gt_bboxes = extract_ground_truth_bboxes(labels, anchors, scale_idx=2, grid_size=predictions[2].shape[2], device=device)\n","            \n","            for img_idx in range(len(images)):\n","                # Filter predictions using NMS\n","                filtered_bboxes = apply_nms(batch_bboxes[img_idx], iou_threshold, conf_threshold)\n","\n","                # Store filtered predictions\n","                for bbox in filtered_bboxes:\n","                    predicted_bboxes.append([image_idx] + bbox)\n","\n","                # Store ground truth bboxes above confidence threshold\n","                for gt_bbox in gt_bboxes[img_idx]:\n","                    if gt_bbox[1] > conf_threshold:\n","                        ground_truth_bboxes.append([image_idx] + gt_bbox)\n","\n","                image_idx += 1\n","\n","    model.train()  # Reset model to training mode\n","    return predicted_bboxes, ground_truth_bboxes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def map_evaluation(predicted_boxes, actual_boxes, num_classes, iou_threshold=0.5):\n","    \"\"\"\n","    This function calculates the mean Average Precision (mAP) for object detection models. \n","    mAP is a metric that evaluates how well the model predicts object locations and classifications.\n","\n","    Parameters:\n","    - predicted_boxes: List of predicted bounding boxes, where each entry is a tuple \n","      (image_index, class_index, confidence_score, x, y, width, height).\n","    - actual_boxes: List of ground truth bounding boxes, where each entry is a tuple \n","      (image_index, class_index, x, y, width, height).\n","    - num_classes: Total number of object classes.\n","    - iou_threshold: Intersection over Union (IoU) threshold for determining whether a prediction \n","      is a true positive (default is 0.5).\n","\n","    Returns:\n","    - Mean Average Precision (mAP) for the given predictions and ground truths.\n","    \"\"\"\n","\n","    # List to store average precision for each class\n","    class_avg_precisions = []\n","\n","    # Ensure numerical stability by adding a small epsilon value to avoid division by zero\n","    epsilon = 1e-6\n","\n","    # Organize ground truths and predictions by class in dictionaries\n","    class_wise_predictions = defaultdict(list)\n","    class_wise_ground_truths = defaultdict(list)\n","\n","    # Populate class-wise predictions dictionary\n","    for pred in predicted_boxes:\n","        class_wise_predictions[pred[1]].append(pred)\n","\n","    # Populate class-wise ground truths dictionary\n","    for gt in actual_boxes:\n","        class_wise_ground_truths[gt[1]].append(gt)\n","\n","    # Iterate over each class to calculate average precision\n","    for class_idx in range(num_classes):\n","        # Get predictions and ground truths for the current class\n","        predictions = class_wise_predictions[class_idx]\n","        ground_truths = class_wise_ground_truths[class_idx]\n","\n","        # If there are no ground truths for this class, skip to the next class\n","        if len(ground_truths) == 0:\n","            continue\n","\n","        # Organize ground truths by image index\n","        gt_image_dict = defaultdict(list)\n","        for gt in ground_truths:\n","            gt_image_dict[gt[0]].append(gt)\n","\n","        # Initialize list to store true positives (TP) and false positives (FP)\n","        tp_fp_pairs = []\n","\n","        # Process each prediction for this class, sorted by confidence score (highest to lowest)\n","        for pred in sorted(predictions, key=lambda x: x[2], reverse=True):\n","            img_idx = pred[0]  # Image index\n","            best_iou = 0  # Initialize the best IoU\n","            best_gt_idx = -1  # Index of the best ground truth match\n","\n","            # Compare prediction with each ground truth in the same image\n","            for gt_idx, gt in enumerate(gt_image_dict[img_idx]):\n","                # Compute IoU between the predicted box and the ground truth box\n","                iou = compute_iou_boxes(\n","                    torch.tensor(pred[3:]),\n","                    torch.tensor(gt[3:])\n","                )\n","\n","                # If this IoU is the best so far, update the best IoU and the index of the best match\n","                if iou > best_iou:\n","                    best_iou = iou\n","                    best_gt_idx = gt_idx\n","\n","            # If the best IoU exceeds the threshold, it is considered a match\n","            if best_iou > iou_threshold:\n","                # Check if this ground truth was already matched to a prediction\n","                if not gt_image_dict[img_idx][best_gt_idx][2]:  # If not matched yet\n","                    tp_fp_pairs.append((1, 0))  # True Positive\n","                    gt_image_dict[img_idx][best_gt_idx][2] = 1  # Mark this GT as matched\n","                else:\n","                    tp_fp_pairs.append((0, 1))  # False Positive due to multiple matches with the same GT\n","            else:\n","                tp_fp_pairs.append((0, 1))  # False Positive due to low IoU\n","\n","        # Convert the list of TP/FP pairs to a 2D tensor\n","        if len(tp_fp_pairs) > 0:\n","            tp_fp_pairs = torch.tensor(tp_fp_pairs, dtype=torch.float32)\n","        else:\n","            tp_fp_pairs = torch.zeros((0, 2))\n","\n","        # Separate true positives and false positives\n","        true_positives = tp_fp_pairs[:, 0]\n","        false_positives = tp_fp_pairs[:, 1]\n","\n","        # Calculate cumulative sums of true positives and false positives\n","        tp_cumsum = torch.cumsum(true_positives, dim=0)\n","        fp_cumsum = torch.cumsum(false_positives, dim=0)\n","\n","        # Calculate recall and precision values\n","        recalls = tp_cumsum / (len(ground_truths) + epsilon)\n","        precisions = tp_cumsum / (tp_cumsum + fp_cumsum + epsilon)\n","\n","        # Add a point (1,0) to the precision-recall curve for easier calculation\n","        precisions = torch.cat((torch.tensor([1]), precisions))\n","        recalls = torch.cat((torch.tensor([0]), recalls))\n","\n","        # Compute the average precision (AP) for this class by integrating the area under the precision-recall curve\n","        ap = torch.trapz(precisions, recalls)\n","        class_avg_precisions.append(ap)\n","\n","    # Return the mean of the average precisions for all classes\n","    return sum(class_avg_precisions) / len(class_avg_precisions) if class_avg_precisions else 0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def validate_func(val_loader, loss_func, model, scaled_anchors):\n","    \"\"\"\n","    This function performs validation on a given validation dataset loader. It evaluates the model's performance\n","    by calculating the average validation loss across all batches in the validation dataset.\n","\n","    Parameters:\n","    - val_loader: DataLoader object for the validation dataset.\n","    - loss_func: The loss function used to compute the loss for model predictions.\n","    - model: The YOLOv3 model being evaluated.\n","    - scaled_anchors: The anchors scaled according to the feature map sizes.\n","\n","    Returns:\n","    - mean_val_loss: The average validation loss over all batches.\n","    \"\"\"\n","\n","    model.eval()  # Set the model to evaluation mode, disabling dropout and batch normalization layers\n","    val_loss_list = []  # Initialize a list to store the loss values for each batch\n","    \n","    # Disable gradient computation during validation to save memory and computation\n","    with torch.no_grad():  \n","        # Iterate over the validation dataset by batch\n","        for i_batch, batch in enumerate(val_loader):\n","            img = batch['image']  # Extract the images from the current batch\n","            outputs = batch['outputs']  # Extract the ground truth outputs (targets) from the current batch\n","\n","            img = img.to(device)  # Move the images to the GPU for faster computation\n","            output0, output1, output2 = [output.to(device) for output in outputs]  # Move ground truth outputs to the GPU\n","\n","            # Perform a forward pass through the model to obtain predictions\n","            model_outputs = model(img)  # Get the model's predictions\n","            \n","            total_loss = 0  # Initialize the total loss for this batch\n","            \n","            # Calculate the total loss across different scales and anchors\n","            for output, target, anchor in zip(model_outputs, [output0, output1, output2], scaled_anchors):\n","                total_loss += loss_func(output, anchor, target)  # Compute the loss for each scale and accumulate\n","\n","            val_loss_list.append(total_loss.item())  # Store the loss value for the current batch\n","\n","    # Calculate the mean validation loss over all batches\n","    mean_val_loss = sum(val_loss_list) / len(val_loss_list)\n","    return mean_val_loss  # Return the mean validation loss"]},{"cell_type":"markdown","metadata":{},"source":["# Step 3: Data Loader\n","\n","This section defines our custom dataset loader using the `torch.utils.data.Dataset` class. It handles the loading of images and their corresponding labels from the dataset. The labels are stored in `.txt` files, where each file has the same name as its associated image. Each line in a label file contains the class index (or class label) and the bounding box coordinates for objects in the image. These bounding box coordinates are normalized between 0 and 1. \n","\n","### Data Transformation\n","\n","The images are first padded to maintain their aspect ratio and then resized to 416x416 pixels, which is the input size expected by the YOLO model. The bounding box coordinates are adjusted accordingly to match the resized image dimensions. During training, data augmentation techniques such as random color jittering are applied to enhance model robustness. For validation or testing, only resizing and normalization are performed to maintain consistency."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class YOLODataset(Dataset):\n","    def __init__(self, root, csv_file, anchors, img_size=416, max_objects=100, ignore_thresh = 0.7, grid_sizes = [13,26,52], train=True):\n","        self.data = pd.read_csv(os.path.join(root, csv_file)) # csv file should have image name and label name\n","        self.img_dir = os.path.join(root, 'images') # image directory\n","        self.label_dir = os.path.join(root, 'labels') # label directory\n","        self.img_size = img_size # image size\n","        self.max_objects = max_objects # maximum number of objects in an image\n","        self.ignore_thresh = ignore_thresh # ignore threshold\n","        self.grid_sizes = grid_sizes # grid sizes\n","        self.train = train # train or test/validation\n","\n","        flattened_anchors = [anchor for scale in anchors for anchor in scale]\n","        self.anchors = torch.tensor(flattened_anchors) # tensor of anchors\n","        self.total_anchors = self.anchors.shape[0]  # total number of anchors\n","        self.anchors_per_scale = self.total_anchors // len(grid_sizes)  # number of anchors per scale\n","\n","        # transform images differently for training and testing/validation.\n","        # Training images are augmented.\n","        if self.train:\n","            self.transform = transforms.Compose([\n","                transforms.Resize((img_size, img_size)),\n","                transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n","            ])\n","        else:\n","            self.transform = transforms.Compose([\n","                transforms.Resize((img_size, img_size)),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n","            ])\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        # ------------------------------- Image part -------------------------------\n","        img_path = os.path.join(self.img_dir, self.data.iloc[index, 0]) # image path for the given index\n","        image = Image.open(img_path).convert('RGB') # open image\n","        original_width, original_height = image.size # original image size\n","\n","        pad_width = 0\n","        pad_height = 0\n","        max_edge = max(original_width, original_height) # maximum edge of the image\n","\n","        # pad the image to make it square\n","        if original_width < max_edge:\n","            pad_width = (max_edge - original_width)\n","        elif original_height < max_edge:\n","            pad_height = (max_edge - original_height)\n","\n","        image = transforms.Pad((0, 0, pad_width, pad_height))(image) # pad image\n","        image = self.transform(image) # transform image\n","\n","        # ------------------------------- Label part -------------------------------\n","        label_path = os.path.join(self.label_dir, self.data.iloc[index, 1]) # label path for the given index\n","\n","        # read the label file if it exists and extract bounding boxes\n","        if os.path.exists(label_path) and os.path.getsize(label_path) > 0:\n","            bboxes = np.loadtxt(label_path, delimiter=\" \", ndmin=2)\n","            if bboxes.ndim == 1:\n","                bboxes = bboxes[np.newaxis, :]\n","        else:\n","            bboxes = np.zeros((0, 5))\n","\n","        # limit the number of objects in an image\n","        if len(bboxes) > self.max_objects:\n","            bboxes = bboxes[:self.max_objects]\n","\n","\n","        scale_x = self.img_size / (original_width + pad_width) # scale factor for x\n","        scale_y = self.img_size / (original_height + pad_height) # scale factor for y\n","\n","        bboxes[:, 1] = (bboxes[:, 1] * original_width) * scale_x / self.img_size  # x_center\n","        bboxes[:, 2] = (bboxes[:, 2] * original_height) * scale_y / self.img_size # y_center\n","        bboxes[:, 3] = (bboxes[:, 3] * original_width) * scale_x / self.img_size # width\n","        bboxes[:, 4] = (bboxes[:, 4] * original_height) * scale_y / self.img_size # height\n","\n","        # Initialize an empty list to store the tensors\n","        output_tensors = []\n","\n","        # Loop through each grid dimension\n","        for grid_size in self.grid_sizes:\n","            # Create a zero-filled tensor for each grid dimension\n","            # dimension 6 stands for : [probability, x, y, width, height, class_label]\n","            zero_tensor = torch.zeros((self.anchors_per_scale, grid_size, grid_size, 6))\n","\n","            # Append the tensor to the output_tensors list\n","            output_tensors.append(zero_tensor)\n","\n","        for box in bboxes:\n","            # Extract the coordinates of the bounding box\n","            class_label, x, y, width, height = box\n","\n","            bbox_tensor = torch.tensor([1, x, y, width, height, class_label])\n","\n","            # Calculate the iou score for the bounding box and the anchors\n","            ious = compute_iou_sizes(bbox_tensor[3:5],self.anchors)\n","\n","            # sort the anchors based on the IoU score\n","            ious_sorted_indx = ious.argsort(descending=True, dim=0)\n","\n","            is_scale_handled = [False for _ in range(len(self.grid_sizes))]\n","\n","            for iou_indx in ious_sorted_indx:\n","                scale_indx = iou_indx // self.anchors_per_scale\n","                anchor_for_indx =  iou_indx % self.anchors_per_scale\n","\n","                grid_size = self.grid_sizes[scale_indx]\n","\n","                # Calculate the grid cell for the bounding box\n","                grid_x = int(x * grid_size)\n","                grid_y = int(y * grid_size)\n","\n","                anchor_probability = output_tensors[scale_indx][anchor_for_indx, grid_y, grid_x, 0]\n","\n","                if anchor_probability == 0 :\n","                    if is_scale_handled[scale_indx] == False:\n","                        # Update the tensor with the bounding box with the relative coordinates to the grid cell\n","                        bbox_tensor[0] = 1\n","                        bbox_tensor[1] = x * grid_size - grid_x\n","                        bbox_tensor[2] = y * grid_size - grid_y\n","                        bbox_tensor[3] = width * grid_size\n","                        bbox_tensor[4] = height * grid_size\n","                        bbox_tensor[5] = int(class_label) # for security cast to int\n","\n","                        output_tensors[scale_indx][anchor_for_indx, grid_y, grid_x] = bbox_tensor\n","                        is_scale_handled[scale_indx] = True # mark the scale as handled\n","\n","                    elif ious[iou_indx] > self.ignore_thresh:\n","                        output_tensors[scale_indx][anchor_for_indx, grid_y, grid_x, 0] = -1 # ignore the anchor\n","\n","        return {\n","            'image': image,\n","            'bboxes': torch.tensor(bboxes, dtype=torch.float32),\n","            'outputs': output_tensors,\n","        }\n"]},{"cell_type":"markdown","metadata":{},"source":["## Test the data loader\n","The following code tests the data loader. It loads the dataset and displays the first image with the bounding boxes of the objects in 2 ways. First it uses directly the bounding boxes retrieved from the label file and the second one uses the output tensor that contains the offset information for each scale and each anchor taken and which is the tensor that will be used for training."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if __name__ == '__main__':\n","\n","    dataset = YOLODataset(root=root_dir, csv_file='train.csv', anchors=normalized_anchors, train=True)\n","    loader = DataLoader(dataset, batch_size=1, shuffle=True)\n","    class_labels = load_classes(names_path)\n","\n","\n","    i = 0\n","    for data in loader:\n","        boxes = []\n","        y = data['outputs']\n","\n","        for i in range(y[0].shape[1]):\n","            anchor = scaled_anchors[i]\n","            boxes += grid_to_bboxes(y[i], anchors=anchor, grid_dim=y[i].shape[2])[0]\n","\n","        boxes = apply_nms_manual(boxes, 0.99, 0.99)\n","        boxes2 = torch.tensor(boxes, dtype=torch.float32)\n","        boxes = transform_boxes(torch.tensor(boxes, dtype=torch.float32))\n","        display_image_with_boxes(data, class_labels) # display the boxes\n","\n","        data['bboxes'] = boxes\n","        display_image_with_boxes(data, class_labels) # display the boxes retrieved from the offsets and anchors\n","\n","        i += 1\n","        if i > 0:\n","            break\n"]},{"cell_type":"markdown","metadata":{},"source":["## Batch loader\n","\n","Since we will be handling only batches, we made a code to test the batches and their sizes if everything matches. The code sets up data loaders for training and test datasets of images using PyTorch, specifically for a YOLOv3 object detection model. It initializes data loaders with a batch size of 8 and an image dimension of 416x416. The training loop iterates over batches, printing out the shape of image tensors and bounding boxes for the first three batches to test the loading and processing functionality."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# This function defines a custom collate function for use with a PyTorch DataLoader.\n","# The collate function is responsible for combining individual samples into a batch\n","# that can be fed into the model during training or inference.\n","\n","def custom_collate_fn(batch):\n","    # Extract the 'image' and 'outputs' fields from each item in the batch.\n","    images = [item['image'] for item in batch]\n","    outputs = [item['outputs'] for item in batch]\n","\n","    # Stack the list of images into a single tensor along a new dimension (dim=0),\n","    # creating a batch of images.\n","    images = torch.stack(images, dim=0)\n","\n","    # Initialize a list to hold the stacked outputs for each grid size and anchor.\n","    outputs_stacked = []\n","    for i in range(len(outputs[0])):  # Assuming the same structure for each batch\n","        # For each grid size and anchor, stack the corresponding outputs across the batch.\n","        outputs_for_scale = torch.stack([output[i] for output in outputs], dim=0)\n","        outputs_stacked.append(outputs_for_scale)\n","\n","    # Return a dictionary containing the stacked images and the stacked outputs,\n","    # ready to be used as input for the model.\n","    return {'image': images, 'outputs': outputs_stacked}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["batch_size2 = 8\n","inp_dim = 416\n","\n","# Use custom collate function in DataLoader\n","dataloaders = {\n","    'train': DataLoader(YOLODataset(root=root_dir, csv_file='train.csv', img_size=inp_dim, anchors=normalized_anchors), batch_size=batch_size2, shuffle=True, collate_fn=custom_collate_fn),\n","}\n","\n","for i_batch, sample_batched in enumerate(dataloaders[\"train\"]):\n","    input_images_batch = sample_batched['image']\n","    outputs_batch = sample_batched['outputs']\n","\n","    print(f\"Batch {i_batch}:\")\n","    print(f\"Image Tensor Shape: {input_images_batch.shape}\")\n","    print(f\"Outputs Shape: {[output.shape for output in outputs_batch]}\")\n","\n","    if i_batch == 3:\n","        break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define a function to create and return the data loaders for training, validation and testing.\n","# The function takes the paths to the training, validation and testing CSV files as input.\n","\n","def get_loaders(train_csv_path, test_csv_path, val_csv_path=None ):\n","    \n","    # Create the training dataset using the specified CSV file, image size, and anchors.\n","    train_dataset = YOLODataset(root=root_dir, csv_file=train_csv_path, img_size=image_size, anchors=normalized_anchors)\n","    \n","    # Create the validation dataset using the specified CSV file, image size and anchors\n","    if val_csv_path != None:\n","        val_dataset = YOLODataset(root=root_dir, csv_file=val_csv_path, img_size=image_size, anchors=normalized_anchors, train=False)\n","    \n","    # Create the testing dataset using the specified CSV file, image size, and anchors.\n","    test_dataset = YOLODataset(root=root_dir, csv_file=test_csv_path, img_size=image_size, anchors=normalized_anchors, train=False)\n","\n","    # Create the data loader for the training dataset.\n","    # The DataLoader handles batching, shuffling, and parallel loading of data.\n","    train_loader = DataLoader(\n","        dataset=train_dataset,\n","        batch_size=batch_size,         # Set the batch size for training\n","        num_workers=num_workers,       # Number of subprocesses to use for data loading\n","        pin_memory=pin_memory,         # If True, the data loader will copy Tensors into CUDA pinned memory before returning them\n","        shuffle=True,                  # Shuffle the data at the start of each step\n","        drop_last=False,               # Do not drop the last incomplete batch\n","        collate_fn=custom_collate_fn   # Use the custom collate function to handle data formatting\n","    )\n","    \n","    # Create the data loader for the validation dataset.\n","    # Similar to the training loader but without data shuffling.\n","    if val_csv_path != None:\n","        val_loader = DataLoader(\n","            dataset=val_dataset,\n","            batch_size=batch_size,         # Set the batch size for testing\n","            num_workers=num_workers,       # Number of subprocesses to use for data loading\n","            pin_memory=pin_memory,         # If True, the data loader will copy Tensors into CUDA pinned memory before returning them\n","            shuffle=False,                 # Do not shuffle the data\n","            drop_last=False,               # Do not drop the last incomplete batch\n","            collate_fn=custom_collate_fn   # Use the custom collate function to handle data formatting\n","        )\n","    else:\n","        val_loader = None\n","\n","    # Create the data loader for the testing dataset.\n","    # Similar to the training loader but without data shuffling.\n","    test_loader = DataLoader(\n","        dataset=test_dataset,\n","        batch_size=batch_size,         # Set the batch size for testing\n","        num_workers=num_workers,       # Number of subprocesses to use for data loading\n","        pin_memory=pin_memory,         # If True, the data loader will copy Tensors into CUDA pinned memory before returning them\n","        shuffle=False,                 # Do not shuffle the data\n","        drop_last=False,               # Do not drop the last incomplete batch\n","        collate_fn=custom_collate_fn   # Use the custom collate function to handle data formatting\n","    )\n","\n","    # Return the training and testing data loaders\n","    return train_loader, val_loader, test_loader"]},{"cell_type":"markdown","metadata":{},"source":["# Step 4 : Build the model\n","Now that we have set up the data, we can attack the model. The model will be based on the darknet 53 architecture and so the first thing to do is to implement the building blocks of this architecture and then integrate them into the YOLOv3 architecture."]},{"cell_type":"markdown","metadata":{},"source":["## Darknet 53"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define a Convolution Block class, which is a common building block in CNN architectures.\n","# This block consists of a convolutional layer followed by batch normalization and \n","# a Leaky ReLU activation function. It is used to extract features from input data.\n","\n","class ConvBlock(nn.Module):\n","    # Initialize the ConvBlock with the given input and output channels, and any additional arguments.\n","    def __init__(self, input_channels, output_channels, **keyword_args):\n","        super().__init__()\n","        # Define a 2D convolutional layer with the specified input and output channels.\n","        # The bias is set to False since batch normalization handles the bias.\n","        self.convolution = nn.Conv2d(input_channels, output_channels, bias=False, **keyword_args)\n","        # Define a batch normalization layer to normalize the output of the convolutional layer,\n","        # which helps in stabilizing and accelerating the training process.\n","        self.bn = nn.BatchNorm2d(output_channels)\n","        # Define a Leaky ReLU activation function with a negative slope of 0.1,\n","        # which allows a small gradient when the input is negative, preventing dead neurons.\n","        self.leaky = nn.LeakyReLU(negative_slope=0.1)\n","\n","    # Define the forward pass, which is the computation performed at each call of the block.\n","    def forward(self, x):\n","        # Apply the convolutional layer, followed by batch normalization, \n","        # and then the Leaky ReLU activation function to the input tensor x.\n","        return self.leaky(self.bn(self.convolution(x)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define a Residual Block class, a fundamental component in many deep neural networks, such as Darknet 53 in this case.\n","# This block helps in training very deep networks by allowing gradients to flow through \n","# skip connections, which mitigate the vanishing gradient problem.\n","\n","class ResBlock(nn.Module):\n","    # Initialize the Residual Block with the specified number of channels, whether to use skip connections,\n","    # and the number of times the block should be repeated.\n","    def __init__(self, channels, skip_connection=True, num_repeats=1):\n","        super().__init__()\n","\n","        # Define the layers that make up the residual block.\n","        # The block consists of a sequence of convolutional layers, batch normalization, and Leaky ReLU activation.\n","        self.layers = nn.ModuleList()\n","        for _ in range(num_repeats):\n","            self.layers += [\n","                nn.Sequential(\n","                    nn.Conv2d(channels, channels // 2, kernel_size=1),  # 1x1 convolution to reduce dimensionality\n","                    nn.BatchNorm2d(channels // 2),\n","                    nn.LeakyReLU(0.1),\n","                    nn.Conv2d(channels // 2, channels, kernel_size=3, padding=1),  # 3x3 convolution to restore original dimensionality\n","                    nn.BatchNorm2d(channels),\n","                    nn.LeakyReLU(0.1)\n","                )\n","            ]\n","\n","        # Store the number of repeats and whether to use skip connections as instance variables.\n","        self.num_repeats = num_repeats\n","        self.skip_connection = skip_connection\n","\n","    # Define the forward pass, which processes the input through each layer of the residual block.\n","    def forward(self, x):\n","        # For each layer in the block, apply the layer to the input.\n","        # If skip connections are enabled, add the original input (residual) to the output of the layer.\n","        for layer in self.layers:\n","            if self.skip_connection:\n","                x = x + layer(x)  # Skip connection: add input to the output of the layer\n","            else:\n","                x = layer(x)  # No skip connection: simply pass the output of the layer\n","        return x  # Return the final output after processing through all layers"]},{"cell_type":"markdown","metadata":{},"source":["## YOLOv3 architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define a YOLOScaleHead class, which is a part of the YOLO model architecture.\n","# This block is responsible for predicting bounding boxes, objectness scores, and class probabilities \n","# for objects at a specific scale in the image.\n","\n","class YOLOScaleHead(nn.Module):\n","    # Initialize the YOLO Scale Head block with the given input channels and number of classes.\n","    def __init__(self, input_channels, number_classes):\n","        super().__init__()\n","\n","        # Define the layers in the YOLO scale head block.\n","        # The block consists of two convolutional layers:\n","        # - The first layer increases the number of channels, followed by batch normalization and Leaky ReLU activation.\n","        # - The second layer reduces the number of channels to match the required output format.\n","        self.prediction = nn.Sequential(\n","            nn.Conv2d(input_channels, 2 * input_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(2 * input_channels),\n","            nn.LeakyReLU(0.1),\n","            nn.Conv2d(2 * input_channels, (number_classes + 5) * 3, kernel_size=1),\n","        )\n","        self.number_classes = number_classes\n","\n","    # Implement the forward pass, which processes the input through the defined layers \n","    # and adjusts the output to match the target format:\n","    # (batch_size, 3, grid_size, grid_size, number_classes + 5).\n","    # This format includes predictions for 3 anchor boxes, each with objectness scores, bounding box attributes,\n","    # and class probabilities.\n","    def forward(self, x):\n","        return (\n","            self.prediction(x)\n","            .view(x.size(0), 3, self.number_classes + 5, x.size(2), x.size(3))\n","            .permute(0, 1, 3, 4, 2)  # Rearrange the dimensions to match the desired output shape\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the YOLOv3 model configuration\n","# C = Convolution Block, R = Residual Block, S = Scale Head Block, U = Upsampling Block\n","\n","cfg = [\n","    [\"C\", 32, 3, 1, 1],\n","    [\"C\", 64, 3, 2, 1],\n","    [\"R\", 1],\n","    [\"C\", 128, 3, 2, 1],\n","    [\"R\", 2],\n","    [\"C\", 256, 3, 2, 1],\n","    [\"R\", 8],\n","    [\"C\", 512, 3, 2, 1],\n","    [\"R\", 8],\n","    [\"C\", 1024, 3, 2, 1],\n","    [\"R\", 4],\n","    [\"C\", 512, 1, 1, 0],\n","    [\"C\", 1024, 3, 1, 1],\n","    [\"S\"],\n","    [\"C\", 256, 1, 1, 0],\n","    [\"U\"],\n","    [\"C\", 256, 1, 1, 0],\n","    [\"C\", 512, 3, 1, 1],\n","    [\"S\"],\n","    [\"C\", 128, 1, 1, 0],\n","    [\"U\"],\n","    [\"C\", 128, 1, 1, 0],\n","    [\"C\", 256, 3, 1, 1],\n","    [\"S\"],\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the YOLOv3 class, which is the main architecture for the YOLOv3 object detection model.\n","# This class encapsulates the entire model, including the layers and the forward pass logic.\n","\n","class YOLO_v3(nn.Module):\n","    # Initialize the YOLOv3 model with the specified input channels (default 3 for RGB images)\n","    # and the number of object classes (default 80, typical for the COCO dataset or 20 for PASCAL_VOC).\n","    def __init__(self, input_channels=3, number_classes=80):\n","        super().__init__()\n","        self.number_classes = number_classes\n","        self.input_channels = input_channels\n","\n","        # Create the layers for the YOLOv3 model, defined by the 'create_layers' method.\n","        self.layers = self.create_layers()\n","\n","    # Define the forward pass for YOLOv3, which handles routing connections, \n","    # scale predictions, and passing the input through the network.\n","    def forward(self, x):\n","        outputs = []  # List to store the outputs for each scale (13x13, 26x26, 52x52).\n","        skip_connections = []  # List to store skip connections for later use in upsampling.\n","\n","        for layer in self.layers:\n","            if isinstance(layer, YOLOScaleHead):\n","                # If the layer is a YOLOScaleHead block, compute the output and store it.\n","                outputs.append(layer(x))\n","                continue\n","\n","            # Pass the input through the current layer.\n","            x = layer(x)\n","\n","            if isinstance(layer, ResBlock) and layer.num_repeats == 8:\n","                # Store the output of the ResBlock with 8 repeats for later use in skip connections.\n","                skip_connections.append(x)\n","\n","            elif isinstance(layer, nn.Upsample):\n","                # If the layer is an upsampling layer, concatenate it with the last skip connection.\n","                x = torch.cat([x, skip_connections[-1]], dim=1)\n","                skip_connections.pop()  # Remove the used skip connection.\n","\n","        return outputs  # Return the outputs for each scale.\n","\n","    # Define the method to create the layers of the YOLOv3 model based on the provided configuration.\n","    def create_layers(self):\n","        input_channels = self.input_channels  # Track the current number of channels through the layers.\n","        layers = nn.ModuleList()  # Create a list to hold all the layers of the model.\n","\n","        for layer in cfg:\n","\n","            if layer[0] == \"C\":\n","                # Add a ConvBlock layer based on the configuration.\n","                layers.append(ConvBlock(input_channels, layer[1], kernel_size=layer[2], stride=layer[3], padding=layer[4]))\n","                input_channels = layer[1]  # Update the input channels for the next layer.\n","\n","            elif layer[0] == \"R\":\n","                # Add a Residual Block (ResBlock) based on the configuration.\n","                layers.append(ResBlock(input_channels, num_repeats=layer[1]))\n","\n","            elif layer[0] == \"S\":\n","                # Add a Residual Block without skip connections, followed by a ConvBlock and YOLOScaleHead block.\n","                layers.append(ResBlock(input_channels, skip_connection=False, num_repeats=1))\n","                layers.append(ConvBlock(input_channels, input_channels // 2, kernel_size=1, stride=1, padding=0))\n","                layers.append(YOLOScaleHead(input_channels // 2, self.number_classes))\n","                input_channels = input_channels // 2  # Update the input channels for the next layer.\n","\n","            elif layer[0] == \"U\":\n","                # Add an upsampling layer to double the spatial dimensions of the input.\n","                layers.append(nn.Upsample(scale_factor=2))\n","                input_channels = input_channels * 3  # Adjust the input channels after upsampling.\n","\n","        return layers  # Return the list of layers that make up the YOLOv3 model."]},{"cell_type":"markdown","metadata":{},"source":["# Step 5: Define the loss function\n","The YOLOv3 loss is divided into four key components. The **Coordinate Loss** measures the accuracy of predicted bounding box coordinates (x, y, width, height) using Mean Squared Error (MSE), focusing only on boxes that contain objects. The **Objectness Loss** also uses MSE to evaluate the model's confidence in the presence of objects within the predicted bounding boxes, comparing these predictions to Intersection over Union (IoU) scores. The **No Object Loss** applies Binary Cross Entropy (BCE) loss to penalize incorrect predictions of object presence in regions where no objects exist, helping to reduce false positives. Finally, the **Class Loss** uses Cross Entropy loss to assess how well the model predicts the correct class for detected objects, ensuring accurate classification. Together, these components guide the model's learning process to improve detection accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the YOLOv3 loss class, which calculates the different components of the loss function \n","# used to train the YOLOv3 model. The loss consists of coordinate loss, object confidence loss, \n","# no object confidence loss, and class prediction loss.\n","\n","class Yolov3Loss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # Define loss functions for different components\n","        self.mse = nn.MSELoss()  # Mean Squared Error for coordinates and object confidence\n","        self.bce = nn.BCEWithLogitsLoss()  # Binary Cross Entropy with logits for no object confidence\n","        self.cross_entropy = nn.CrossEntropyLoss()  # Cross Entropy loss for class predictions\n","        self.sigmoid_function = nn.Sigmoid()  # Sigmoid function used for converting logits to probabilities\n","\n","        # Define the weights for each loss component\n","        self.loss_class = 1  # Weight for class prediction loss\n","        self.loss_noobj = 1  # Weight for no object confidence loss\n","        self.loss_coord = 1  # Weight for coordinate loss\n","        self.loss_obj = 1  # Weight for object confidence loss\n","\n","    def forward(self, predictions, anchors, output):\n","        # Retrieve the mask indicating where objects are present and where they are not\n","        obj = torch.eq(output[..., 0], 1)\n","        noobj = torch.eq(output[..., 0], 0)\n","\n","        # Reshape the anchors to match the shape of the predictions\n","        anchors = anchors.unsqueeze(0).unsqueeze(2).unsqueeze(3)\n","\n","        # ----------------------------- Coordinates loss ----------------------------- #\n","\n","        # Apply sigmoid to x and y coordinates of predictions to constrain them between 0 and 1\n","        pred_coords_xy = self.sigmoid_function(predictions[..., 1:3])\n","\n","        # Scale the width and height of the ground truth by the anchors and take the log\n","        scaled_wh = torch.log(\n","            torch.clamp(output[..., 3:5] / anchors, min=1e-16)  # Clamp to avoid log(0)\n","        )\n","\n","        # Update the predictions tensor with the transformed coordinates\n","        updated_predictions = torch.cat((pred_coords_xy, predictions[..., 3:5]), dim=-1)\n","\n","        # Update the output tensor with the scaled width and height\n","        updated_output = torch.cat((output[..., 1:3], scaled_wh), dim=-1)\n","        \n","        # Calculate the MSE loss for the bounding box coordinates\n","        coordinates_loss = self.mse(updated_predictions[obj], updated_output[obj])\n","\n","        # ----------------------------- Object loss ----------------------------- #\n","\n","        # Calculate the bounding box predictions for x, y, width, and height\n","        box_xy = self.sigmoid_function(predictions[..., 1:3])  # Sigmoid applied to x, y\n","        box_wh = torch.exp(predictions[..., 3:5]) * anchors   # Exponential applied to width and height, then scaled by anchors\n","\n","        # Concatenate the predictions along the last dimension to form complete bounding boxes\n","        box_predictions = torch.cat((box_xy, box_wh), dim=-1)\n","\n","        # Compute IoU (Intersection over Union) scores between the predicted and ground-truth boxes\n","        iou_values = compute_iou_boxes(box_predictions[obj], output[..., 1:5][obj]).detach()\n","\n","        # Calculate the object confidence loss using MSE between predicted confidence and IoU values\n","        pred_confidence = self.sigmoid_function(predictions[..., 0:1][obj])\n","        actual_confidence = iou_values * output[..., 0:1][obj]\n","\n","        loss_obj = self.mse(pred_confidence, actual_confidence)\n","\n","        # ----------------------------- No object loss ----------------------------- #\n","\n","        # Reshape noobj mask to match the shape of the predictions' objectness scores\n","        noobj_expanded = noobj.unsqueeze(-1)  # This makes the shape (batch_size, num_grids, num_grids, 1)\n","\n","        # Apply the mask to both predictions and output using torch.masked_select to select no-object regions\n","        pred_noobj = torch.masked_select(predictions[..., 0:1], noobj_expanded)\n","        output_noobj = torch.masked_select(output[..., 0:1], noobj_expanded)\n","\n","        # Calculate the BCE loss for regions where no objects are present\n","        no_object_loss = self.bce(pred_noobj, output_noobj)\n","\n","        # ----------------------------- Class loss ----------------------------- #\n","\n","        # Select the class predictions and corresponding ground truth classes where objects are present\n","        predicted_classes = predictions[..., 5:][obj]\n","        ground_truth_classes = output[..., 5][obj].long()\n","\n","        # Calculate the cross entropy loss for class predictions\n","        class_loss = self.cross_entropy(predicted_classes, ground_truth_classes)\n","\n","        # Return the total loss as a weighted sum of all components\n","        return (\n","            self.loss_coord * coordinates_loss  # Weighted coordinate loss\n","            + self.loss_obj * loss_obj          # Weighted object confidence loss\n","            + self.loss_noobj * no_object_loss  # Weighted no object confidence loss\n","            + self.loss_class * class_loss      # Weighted class prediction loss\n","        )"]},{"cell_type":"markdown","metadata":{},"source":["# Step 6: Define the training function\n","The training function consists of processing each batch of data through the model, calculating the loss for multiple scales and anchors, and updating the model's weights using backpropagation. It utilizes mixed precision for efficiency, tracks progress with a real-time progress bar, and manages GPU memory by clearing the CUDA cache after each batch."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the training function that handles the training loop for the YOLOv3 model.\n","# This function processes batches of data, calculates the loss, and updates the model weights.\n","# It also utilizes a progress bar to visually track training progress and loss metrics.\n","\n","def train_func(train_loader, loss_func, model, optimizer, scaler, scaled_anchors):\n","    total_batches = len(train_loader)  # Get the total number of batches in the training loader\n","    console = Console()  # Initialize a console object for rich text display\n","\n","    # Set up a progress bar with various metrics and visual indicators\n","    with Progress(\n","        TextColumn(\"[bold blue]Training...\"),\n","        BarColumn(bar_width=None),\n","        \"[progress.percentage]{task.percentage:>3.0f}%\",\n","        \" | {task.completed}/{task.total}\",\n","        TimeElapsedColumn(),\n","        TimeRemainingColumn(),\n","        \" | Current Loss: {task.fields[current_loss]}\",\n","        \" | Mean Loss: {task.fields[mean_loss]}\",\n","        console=console,\n","    ) as progress:\n","        # Create a progress task to monitor the training progress\n","        task = progress.add_task(\"Training\", total=total_batches, current_loss=\"N/A\", mean_loss=\"N/A\")\n","        loss_list = []  # List to store the loss values for calculating the mean loss\n","\n","        # Loop over each batch in the training data\n","        for i_batch, batch in enumerate(train_loader):\n","            img = batch['image']  # Get the images from the batch\n","            outputs = batch['outputs']  # Get the ground truth outputs\n","\n","            img = img.to(device)  # Move the images to the GPU\n","            output0, output1, output2 = [output.to(device) for output in outputs]  # Move the outputs to the GPU\n","\n","            # Forward pass through the model with automatic mixed precision for efficiency\n","            with torch.cuda.amp.autocast():\n","                model_outputs = model(img)  # Get the model's predictions\n","                total_loss = 0\n","                # Calculate the total loss across the different scales and anchors\n","                for output, target, anchor in zip(model_outputs, [output0, output1, output2], scaled_anchors):\n","                    total_loss += loss_func(output, anchor, target)\n","\n","            # Accumulate the loss values for later analysis\n","            loss_list.append(total_loss.item())\n","\n","            # Backward pass to compute gradients and update model weights\n","            optimizer.zero_grad()  # Clear the gradients\n","            total_loss.backward()  # Compute gradients\n","            optimizer.step()  # Update model parameters\n","\n","            # Update the progress bar with the latest loss metrics\n","            mean_loss = sum(loss_list) / len(loss_list)  # Calculate the mean loss so far\n","            progress.update(task, advance=1, current_loss=f\"{loss_list[-1]:.4f}\", mean_loss=f\"{mean_loss:.4f}\")\n","\n","            # Clear the CUDA memory cache to optimize GPU memory usage\n","            torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["# Step 7 : Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class_labels = load_classes(names_path)\n","# create the YOLOv3 model\n","gc.collect()\n","\n","# Empty the CUDA cache\n","torch.cuda.empty_cache()\n","#print_memory_usage()\n","model = YOLO_v3(number_classes=len(class_labels)).to(device)\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n","\n","# Define the scaler for automatic mixed precision\n","scaler = torch.cuda.amp.GradScaler()\n","\n","# Define the loss function\n","loss_func = Yolov3Loss()\n","\n","# Define the train loader and test loader\n","train_loader ,_ , test_loader = get_loaders(train_csv_path, test_csv_path, val_csv_path=None)\n","\n","# Check if there's a snapshot to load\n","start_step = 0\n","if resume_training:  # 'resume_training' is a boolean flag indicating whether to resume training\n","    start_step, _ = load_snapshot(snapshot_file, model, optimizer, scheduler=None, device=device)\n","    print(f\"Resuming training from step {start_step+1}\")\n","\n","for step in range(start_step + 1, num_steps):\n","    print(f\"Step n°: {step + 1}\\n----------------------------\")\n","    train_func(train_loader, loss_func, model, optimizer, scaler, scaled_anchors)\n","\n","    # Saving the model\n","    if save_model:\n","      save_snapshot(model, optimizer, filename=f\"{snapshot_file}\", step=step)\n","    \n","    # Evaluate the model and compute mAP every 3 steps\n","    if step != 0 and step % 3 == 0:\n","        \n","        # Step 0 : Compute validation loss\n","        val_loss = validate_func(test_loader, loss_func, model, scaled_anchors)\n","        print(f\"Validation Loss step {step + 1}: {val_loss:.4f}\")\n","        \n","        # Step 1: Evaluate model accuracy on the validation dataset\n","        evaluate_model_accuracy(model, test_loader, conf_threshold, iou_threshold)\n","\n","        # Step 2: Gather predicted and ground truth bounding boxes for evaluation\n","        #predicted_bboxes, ground_truth_bboxes = gather_evaluation_bboxes(\n","        #    test_loader, model, iou_threshold, conf_threshold, anchors, device=device\n","        #)\n","\n","        # Step 3: Calculate the mean Average Precision (mAP) for the current step using the predicted and ground truth boxes\n","        #map_eval_50 = map_evaluation(predicted_bboxes, ground_truth_bboxes, len(class_labels), iou_threshold = 0.5)\n","        #map_eval_75 = map_evaluation(predicted_bboxes, ground_truth_bboxes, len(class_labels), iou_threshold = 0.75)\n","\n","        # Step 5: Print the computed mAP values\n","        #print(f\"MAP_50: {map_eval_50.detach()}\")  # Detach the tensor from the computation graph before printing\n","        \n","        #print(f\"MAP_75: {map_eval_75.detach()}\")  # Detach the tensor from the computation graph before printing\n","\n","        # Step 6: Return the model to training mode after evaluation\n","        model.train()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Step 8 : Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load the class labels\n","labels_list = load_classes(names_path)\n","\n","# Initialize the YOLOv3 model with the number of classes\n","yolo_model = YOLO_v3(number_classes=len(labels_list)).to(device)\n","\n","# Set up the optimizer\n","optimizer = optim.Adam(yolo_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","\n","# Set up the scaler for mixed precision training\n","amp_scaler = torch.cuda.amp.GradScaler()\n","\n","# Initialize the loss function\n","yolo_loss_function = Yolov3Loss()\n","\n","# Flag to determine whether to load a pre-trained model\n","resume_training = True\n","\n","# Load a checkpoint if resuming from a previous state\n","if resume_training:\n","    load_snapshot(snapshot_file, yolo_model, optimizer, scheduler=None, lr=learning_rate, device=device)\n","\n","# Create a dataset and data loader for testing\n","validation_dataset = YOLODataset(\n","    root=root_dir, \n","    csv_file=test_csv_path, \n","    img_size=image_size, \n","    anchors=normalized_anchors, \n","    train=False\n",")\n","\n","validation_loader = DataLoader(\n","    dataset=validation_dataset,\n","    batch_size=1,  # Batch size for testing\n","    num_workers=num_workers,  # Number of data loading workers\n","    pin_memory=pin_memory,  # Use pinned memory for faster GPU transfer\n","    shuffle=True,  # Shuffle the data (although typically, we might not shuffle for validation/testing)\n","    drop_last=False  # Include all batches, even if the last one is incomplete\n",")\n","\n","# Process a sample batch from the validation data loader\n","for idx, batch in enumerate(validation_loader):\n","    if idx == 1:\n","        break\n","\n","    input_images = batch['image'].to(device)\n","    \n","    yolo_model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():\n","        # Generate predictions from the model\n","        predictions = yolo_model(input_images)\n","\n","        # Initialize list to collect bounding boxes for each image\n","        predicted_boxes = [[] for _ in range(input_images.shape[0])]\n","\n","        # Extract bounding boxes for each prediction scale\n","        for scale_idx in range(3):\n","            batch_size, num_anchors, grid_size, _, _ = predictions[scale_idx].shape\n","            anchors_at_scale = scaled_anchors[scale_idx]\n","            boxes_at_scale = pred_to_bboxes(predictions[scale_idx], anchors_at_scale, grid_dim=grid_size)\n","\n","            for img_idx, box in enumerate(boxes_at_scale):\n","                predicted_boxes[img_idx] += box\n","        \n","    yolo_model.train()  # Switch back to training mode\n","    \n","    print(\"Expected result ====>\")\n","    display_image_with_boxes(batch, labels_list)\n","\n","    # Apply non-max suppression and plot the results for each image in the batch\n","    for img_idx in range(batch_size):\n","        nms_filtered_boxes = apply_nms(predicted_boxes[img_idx], iou_thresh=0.5, conf_thresh=0.7)\n","        \n","        # Convert and display the image with bounding boxes\n","        batch['bboxes'] = transform_boxes(torch.tensor(nms_filtered_boxes, dtype=torch.float32))\n","        print(\"Our result ===>\")\n","        display_image_with_boxes(batch, labels_list)\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5503858,"sourceId":9117936,"sourceType":"datasetVersion"},{"datasetId":5512328,"sourceId":9129951,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
